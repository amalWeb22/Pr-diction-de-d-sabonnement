{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caractéristiques sélectionnées : Index(['International plan', 'Voice mail plan', 'Number vmail messages',\n",
      "       'Total day minutes', 'Total day charge', 'Total eve minutes',\n",
      "       'Total eve charge', 'Total night minutes', 'Total night charge',\n",
      "       'Total intl minutes', 'Total intl charge', 'Customer service calls'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Charger les données\n",
    "file_path = \"churn-bigml-80.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Sélectionner les caractéristiques et la cible\n",
    "X = df.drop(columns=[\"Churn\"])  # Caractéristiques\n",
    "y = df[\"Churn\"]  # Cible\n",
    "\n",
    "# Encoder les caractéristiques catégoriques en valeurs numériques\n",
    "encoder = OrdinalEncoder()\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Sélectionner les k meilleures caractéristiques\n",
    "k_best = SelectKBest(score_func=chi2, k= 12)  # Choisissez le nombre de caractéristiques que vous souhaitez conserver\n",
    "X_selected = k_best.fit_transform(X_encoded, y)\n",
    "\n",
    "# Afficher les noms des caractéristiques sélectionnées\n",
    "selected_feature_names = X.columns[k_best.get_support(indices=True)]\n",
    "print(\"Caractéristiques sélectionnées :\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+-----+\n",
      "|features                                                        |Churn|\n",
      "+----------------------------------------------------------------+-----+\n",
      "|[0.0,1.0,25.0,265.1,45.07,197.4,16.78,244.7,11.01,10.0,2.7,1.0] |false|\n",
      "|[0.0,1.0,26.0,161.6,27.47,195.5,16.62,254.4,11.45,13.7,3.7,1.0] |false|\n",
      "|[0.0,0.0,0.0,243.4,41.38,121.2,10.3,162.6,7.32,12.2,3.29,0.0]   |false|\n",
      "|[1.0,0.0,0.0,299.4,50.9,61.9,5.26,196.9,8.86,6.6,1.78,2.0]      |false|\n",
      "|[1.0,0.0,0.0,166.7,28.34,148.3,12.61,186.9,8.41,10.1,2.73,3.0]  |false|\n",
      "|[1.0,0.0,0.0,223.4,37.98,220.6,18.75,203.9,9.18,6.3,1.7,0.0]    |false|\n",
      "|[0.0,1.0,24.0,218.2,37.09,348.5,29.62,212.6,9.57,7.5,2.03,3.0]  |false|\n",
      "|[1.0,0.0,0.0,157.0,26.69,103.1,8.76,211.8,9.53,7.1,1.92,0.0]    |false|\n",
      "|[1.0,1.0,37.0,258.6,43.96,222.0,18.87,326.4,14.69,11.2,3.02,0.0]|false|\n",
      "|[0.0,0.0,0.0,187.7,31.91,163.4,13.89,196.0,8.82,9.1,2.46,0.0]   |false|\n",
      "|[0.0,0.0,0.0,128.8,21.9,104.9,8.92,141.1,6.35,11.2,3.02,1.0]    |false|\n",
      "|[0.0,0.0,0.0,156.6,26.62,247.6,21.05,192.3,8.65,12.3,3.32,3.0]  |false|\n",
      "|[0.0,0.0,0.0,120.7,20.52,307.2,26.11,203.0,9.14,13.1,3.54,4.0]  |false|\n",
      "|[0.0,1.0,27.0,196.4,33.39,280.9,23.88,89.3,4.02,13.8,3.73,1.0]  |false|\n",
      "|[0.0,0.0,0.0,190.7,32.42,218.2,18.55,129.6,5.83,8.1,2.19,3.0]   |false|\n",
      "|[0.0,1.0,33.0,189.7,32.25,212.8,18.09,165.7,7.46,10.0,2.7,1.0]  |false|\n",
      "|[0.0,0.0,0.0,224.4,38.15,159.5,13.56,192.8,8.68,13.0,3.51,1.0]  |false|\n",
      "|[0.0,0.0,0.0,155.1,26.37,239.7,20.37,208.8,9.4,10.6,2.86,0.0]   |false|\n",
      "|[0.0,0.0,0.0,62.4,10.61,169.9,14.44,209.6,9.43,5.7,1.54,5.0]    |true |\n",
      "|[0.0,0.0,0.0,183.0,31.11,72.9,6.2,181.8,8.18,9.5,2.57,0.0]      |false|\n",
      "+----------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Prétraitement et ML avec PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Charger les données\n",
    "file_path = \"churn-bigml-80.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convertir la colonne \"Churn\" en StringType\n",
    "df = df.withColumn(\"Churn\", df[\"Churn\"].cast(\"string\"))\n",
    "\n",
    "# Encoder la variable cible \"Churn\"\n",
    "indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Sélectionner les colonnes pour les caractéristiques\n",
    "selected_features =['International plan', 'Voice mail plan', 'Number vmail messages',\n",
    "       'Total day minutes', 'Total day charge', 'Total eve minutes',\n",
    "       'Total eve charge', 'Total night minutes', 'Total night charge',\n",
    "       'Total intl minutes', 'Total intl charge', 'Customer service calls']\n",
    "     \n",
    "\n",
    "# Supprimer les valeurs manquantes\n",
    "df = df.na.drop()\n",
    "\n",
    "# Encoder les variables catégoriques en numériques\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(df) for col in ['International plan', 'Voice mail plan']]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Assembler les caractéristiques sélectionnées dans un vecteur\n",
    "assembler = VectorAssembler(inputCols=[col+\"_index\" if col in ['International plan', 'Voice mail plan'] else col for col in selected_features],\n",
    "                            outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Afficher le DataFrame résultant\n",
    "df.select(\"features\", \"Churn\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Initialiser le MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "# Calculer les statistiques de résumé et normaliser les caractéristiques\n",
    "scaler_model = scaler.fit(df)\n",
    "df = scaler_model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|     scaled_features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,1.0,0.5,0.75...|  0.0|\n",
      "|[0.0,1.0,0.52,0.4...|  0.0|\n",
      "|[0.0,0.0,0.0,0.69...|  0.0|\n",
      "|[1.0,0.0,0.0,0.85...|  0.0|\n",
      "|[1.0,0.0,0.0,0.47...|  0.0|\n",
      "|[1.0,0.0,0.0,0.63...|  0.0|\n",
      "|[0.0,1.0,0.48,0.6...|  0.0|\n",
      "|[1.0,0.0,0.0,0.44...|  0.0|\n",
      "|[1.0,1.0,0.74,0.7...|  0.0|\n",
      "|[0.0,0.0,0.0,0.53...|  0.0|\n",
      "|[0.0,0.0,0.0,0.36...|  0.0|\n",
      "|[0.0,0.0,0.0,0.44...|  0.0|\n",
      "|[0.0,0.0,0.0,0.34...|  0.0|\n",
      "|[0.0,1.0,0.54,0.5...|  0.0|\n",
      "|[0.0,0.0,0.0,0.54...|  0.0|\n",
      "|[0.0,1.0,0.66,0.5...|  0.0|\n",
      "|[0.0,0.0,0.0,0.63...|  0.0|\n",
      "|[0.0,0.0,0.0,0.44...|  0.0|\n",
      "|[0.0,0.0,0.0,0.17...|  1.0|\n",
      "|[0.0,0.0,0.0,0.52...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.select(\"scaled_features\", \"label\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Classifier AUC: 0.9220522763176806\n",
      "Random Forest AUC: 0.9086241562544877\n",
      "Logistic Regression AUC: 0.8285580927761017\n",
      "Naive Bayes AUC: 0.36227200919144\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports nécessaires pour les modèles de classification et les évaluateurs\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "\n",
    "# Entraîner le modèle GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol='scaled_features', labelCol='label', maxBins=64)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "predictions_gbt = gbt_model.transform(test_data)\n",
    "\n",
    "# Évaluer le modèle GBTClassifier sur l'ensemble de test\n",
    "evaluator_gbt = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
    "eval_result_gbt = evaluator_gbt.evaluate(predictions_gbt)\n",
    "print(\"GBT Classifier AUC:\", eval_result_gbt)\n",
    "\n",
    "# Entraîner le modèle RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol='scaled_features', labelCol='label', maxBins=64)\n",
    "rf_model = rf.fit(train_data)\n",
    "predictions_rf = rf_model.transform(test_data)\n",
    "\n",
    "# Évaluer le modèle RandomForestClassifier sur l'ensemble de test\n",
    "evaluator_rf = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
    "eval_result_rf = evaluator_rf.evaluate(predictions_rf)\n",
    "print(\"Random Forest AUC:\", eval_result_rf)\n",
    "\n",
    "# Entraîner le modèle LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='scaled_features', labelCol='label', maxIter=10)\n",
    "lr_model = lr.fit(train_data)\n",
    "predictions_lr = lr_model.transform(test_data)\n",
    "\n",
    "# Évaluer le modèle LogisticRegression sur l'ensemble de test\n",
    "evaluator_lr = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
    "eval_result_lr = evaluator_lr.evaluate(predictions_lr)\n",
    "print(\"Logistic Regression AUC:\", eval_result_lr)\n",
    "\n",
    "# Entraîner le modèle Naive Bayes\n",
    "nb = NaiveBayes(featuresCol='scaled_features', labelCol='label')\n",
    "nb_model = nb.fit(train_data)\n",
    "predictions_nb = nb_model.transform(test_data)\n",
    "\n",
    "# Évaluer le modèle Naive Bayes sur l'ensemble de test\n",
    "evaluator_nb = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
    "eval_result_nb = evaluator_nb.evaluate(predictions_nb)\n",
    "print(\"Naive Bayes AUC:\", eval_result_nb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gbt_model.save(\"gbt_model2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
